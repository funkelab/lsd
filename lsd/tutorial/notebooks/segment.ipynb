{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLsuqADfVjiw"
      },
      "source": [
        "*  Before starting, click \"Runtime\" in the top panel, select \"Change runtime type\" and then choose \"GPU\"\n",
        "\n",
        "*  This tutorial follows the inference tutorial, and is therefore condensed. Check out the inference tutorial (**inference.ipynb**) if there is any confusion throughout\n",
        "\n",
        "*  Try running each cell consecutively to see what is happening before changing things around\n",
        "\n",
        "*  Some cells are collapsed by default, these are generally utility functions or are expanded by defaullt in a previous tutorial. Double click to expand/collapse\n",
        "\n",
        "*  sometimes colab can be slow, if this happens you may need to restart the runtime. also, you generally can only run one session at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Nvia0v8MVtwz"
      },
      "source": [
        "#@title install packages + repos\n",
        "\n",
        "# packages\n",
        "!pip install matplotlib\n",
        "!pip install scikit-image\n",
        "!pip install scipy\n",
        "!pip install torch\n",
        "!pip install zarr\n",
        "\n",
        "# repos\n",
        "!pip install git+https://github.com/funkelab/daisy.git\n",
        "!pip install git+https://github.com/funkey/gunpowder.git\n",
        "!pip install git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip install git+https://github.com/funkey/waterz.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "r_UMHUJFV86c"
      },
      "source": [
        "#@title fetch model checkpoint\n",
        "\n",
        "!git clone https://github.com/funkelab/lsd.git\n",
        "!cd lsd && git checkout tutorial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OLCL0uUsV_pr"
      },
      "source": [
        "#@title import packages\n",
        "\n",
        "import daisy\n",
        "import gunpowder as gp\n",
        "import h5py\n",
        "import io\n",
        "import logging\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import requests\n",
        "import torch\n",
        "import waterz\n",
        "import zarr\n",
        "\n",
        "from funlib.learn.torch.models import UNet, ConvPass\n",
        "from gunpowder.torch import Predict\n",
        "from scipy.ndimage import label\n",
        "from scipy.ndimage import measurements\n",
        "from scipy.ndimage.filters import maximum_filter\n",
        "from scipy.ndimage.morphology import distance_transform_edt\n",
        "from skimage.segmentation import watershed\n",
        "\n",
        "%matplotlib inline\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "pjmgcsx6WP7D"
      },
      "source": [
        "#@title utility function to view labels\n",
        "\n",
        "# matplotlib uses a default shader\n",
        "# we need to recolor as unique objects\n",
        "\n",
        "def create_lut(labels):\n",
        "\n",
        "    max_label = np.max(labels)\n",
        "\n",
        "    lut = np.random.randint(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            size=(int(max_label + 1), 3),\n",
        "            dtype=np.uint64)\n",
        "\n",
        "    lut = np.append(\n",
        "            lut,\n",
        "            np.zeros(\n",
        "                (int(max_label + 1), 1),\n",
        "                dtype=np.uint8) + 255,\n",
        "            axis=1)\n",
        "\n",
        "    lut[0] = 0\n",
        "    colored_labels = lut[labels]\n",
        "\n",
        "    return colored_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7s0kNrV4WVML"
      },
      "source": [
        "#@title utility  function to download / save data as zarr\n",
        "def create_data(\n",
        "    url, \n",
        "    name, \n",
        "    offset, \n",
        "    resolution,\n",
        "    sections=None,\n",
        "    squeeze=True):\n",
        "\n",
        "  in_f = h5py.File(io.BytesIO(requests.get(url).content), 'r')\n",
        "\n",
        "  raw = in_f['volumes/raw']\n",
        "  labels = in_f['volumes/labels/neuron_ids']\n",
        "  \n",
        "  f = zarr.open(name, 'a')\n",
        "\n",
        "  if sections is None:\n",
        "    sections=range(raw.shape[0]-1)\n",
        "\n",
        "  for i, r in enumerate(sections):\n",
        "\n",
        "    print(f'Writing data for section {r}')\n",
        "\n",
        "    raw_slice = raw[r:r+1,:,:]\n",
        "    labels_slice = labels[r:r+1,:,:]\n",
        "\n",
        "    if squeeze:\n",
        "      raw_slice = np.squeeze(raw_slice)\n",
        "      labels_slice = np.squeeze(labels_slice)\n",
        "\n",
        "    f[f'raw/{i}'] = raw_slice\n",
        "    f[f'labels/{i}'] = labels_slice\n",
        "\n",
        "    f[f'raw/{i}'].attrs['offset'] = offset\n",
        "    f[f'raw/{i}'].attrs['resolution'] = resolution\n",
        "\n",
        "    f[f'labels/{i}'].attrs['offset'] = offset\n",
        "    f[f'labels/{i}'].attrs['resolution'] = resolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvnwUYHBWZF5"
      },
      "source": [
        "# get first section\n",
        "\n",
        "create_data(\n",
        "    'https://cremi.org/static/data/sample_A_20160501.hdf',\n",
        "    'testing_data.zarr',\n",
        "    offset=[0,0],\n",
        "    resolution=[4,4],\n",
        "    sections=[0],\n",
        "    squeeze=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "okAFte1uWevZ"
      },
      "source": [
        "#@title inference wrapper\n",
        "voxel_size = gp.Coordinate((4, 4))\n",
        "\n",
        "input_shape = gp.Coordinate((164, 164))\n",
        "output_shape = gp.Coordinate((124, 124))\n",
        "\n",
        "input_size = input_shape * voxel_size\n",
        "output_size = output_shape * voxel_size\n",
        "\n",
        "# total roi of image to predict on\n",
        "total_roi = gp.Coordinate((1250,1250))*voxel_size\n",
        "\n",
        "num_fmaps=12\n",
        "\n",
        "class MtlsdModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.unet = UNet(\n",
        "            in_channels=1,\n",
        "            num_fmaps=num_fmaps,\n",
        "            fmap_inc_factor=5,\n",
        "            downsample_factors=[\n",
        "                [2, 2],\n",
        "                [2, 2]],\n",
        "            kernel_size_down=[\n",
        "                [[3, 3], [3, 3]],\n",
        "                [[3, 3], [3, 3]],\n",
        "                [[3, 3], [3, 3]]],\n",
        "            kernel_size_up=[\n",
        "                [[3, 3], [3, 3]],\n",
        "                [[3, 3], [3, 3]]])\n",
        "\n",
        "        self.lsd_head = ConvPass(num_fmaps, 6, [[1, 1]], activation='Sigmoid')\n",
        "        self.aff_head = ConvPass(num_fmaps, 2, [[1, 1]], activation='Sigmoid')\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        z = self.unet(input)\n",
        "        lsds = self.lsd_head(z)\n",
        "        affs = self.aff_head(z)\n",
        "\n",
        "        return lsds, affs\n",
        "\n",
        "class WeightedMSELoss(torch.nn.MSELoss):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeightedMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, lsds_prediction, lsds_target, lsds_weights, affs_prediction, affs_target, affs_weights,):\n",
        "\n",
        "        loss1 = super(WeightedMSELoss, self).forward(\n",
        "                lsds_prediction*lsds_weights,\n",
        "                lsds_target*lsds_weights)\n",
        "\n",
        "        loss2 = super(WeightedMSELoss, self).forward(\n",
        "            affs_prediction*affs_weights,\n",
        "            affs_target*affs_weights)\n",
        "        \n",
        "        return loss1 + loss2\n",
        "\n",
        "def predict(\n",
        "    checkpoint,\n",
        "    raw_file,\n",
        "    raw_dataset,\n",
        "    out_file,\n",
        "    out_lsds,\n",
        "    out_affs):\n",
        "  \n",
        "  raw = gp.ArrayKey('RAW')\n",
        "  pred_lsds = gp.ArrayKey('PRED_LSDS')\n",
        "  pred_affs = gp.ArrayKey('PRED_AFFS')\n",
        "\n",
        "  if os.path.exists(out_file):\n",
        "    mode='r+'\n",
        "  else:\n",
        "    mode='w'\n",
        "\n",
        "  of = zarr.open(out_file, mode=mode)\n",
        "  rd = zarr.open(raw_file)[raw_dataset]\n",
        "\n",
        "  for ds in [out_lsds, out_affs]:\n",
        "      if ds not in of:\n",
        "          if 'lsd' in ds:\n",
        "              dims = 6\n",
        "          else:\n",
        "              dims = 2\n",
        "          out_ds = of.create_dataset(\n",
        "                  ds,\n",
        "                  shape= (dims,) + np.squeeze(rd).shape,\n",
        "                  dtype=np.float32)\n",
        "          out_ds.attrs['resolution'] = voxel_size\n",
        "\n",
        "  scan_request = gp.BatchRequest()\n",
        "\n",
        "  scan_request.add(raw, input_size)\n",
        "  scan_request.add(pred_lsds, output_size)\n",
        "  scan_request.add(pred_affs, output_size)\n",
        "\n",
        "  source = gp.ZarrSource(\n",
        "              raw_file,\n",
        "          {\n",
        "              raw: raw_dataset\n",
        "          },\n",
        "          {\n",
        "              raw: gp.ArraySpec(interpolatable=True)\n",
        "          })\n",
        "\n",
        "  model = MtlsdModel()\n",
        "  model.eval()\n",
        "\n",
        "  predict = gp.torch.Predict(\n",
        "      model=model,\n",
        "      checkpoint=checkpoint,\n",
        "      inputs = {\n",
        "                'input': raw\n",
        "      },\n",
        "      outputs = {\n",
        "          0: pred_lsds,\n",
        "          1: pred_affs})\n",
        "  \n",
        "  scan = gp.Scan(scan_request)\n",
        "\n",
        "  write = gp.ZarrWrite(\n",
        "      dataset_names={\n",
        "          pred_lsds: out_lsds,\n",
        "          pred_affs: out_affs},\n",
        "      output_filename=out_file)\n",
        "  \n",
        "  pipeline = source\n",
        "  pipeline += gp.Normalize(raw)\n",
        "  pipeline += gp.Stack(1)\n",
        "  pipeline += predict\n",
        "  pipeline += scan\n",
        "  pipeline += gp.Squeeze([pred_lsds, pred_affs])\n",
        "  pipeline += write\n",
        "\n",
        "  predict_request = gp.BatchRequest()\n",
        "\n",
        "  predict_request.add(raw, total_roi)\n",
        "  predict_request.add(pred_lsds, total_roi)\n",
        "  predict_request.add(pred_affs, total_roi)\n",
        "\n",
        "  with gp.build(pipeline):\n",
        "      pipeline.request_batch(predict_request)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmowwZdfWxHl"
      },
      "source": [
        "checkpoint = 'lsd/lsd/tutorial/notebooks/model_checkpoint_50000' \n",
        "raw_file = 'testing_data.zarr'\n",
        "raw_dataset = 'raw/0'\n",
        "out_file = 'prediction.zarr'\n",
        "out_lsds = 'pred_lsds/0'\n",
        "out_affs = 'pred_affs/0'\n",
        "\n",
        "predict(\n",
        "    checkpoint,\n",
        "    raw_file,\n",
        "    raw_dataset,\n",
        "    out_file,\n",
        "    out_lsds,\n",
        "    out_affs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE9a28voWzn6"
      },
      "source": [
        "test_f = zarr.open('testing_data.zarr')\n",
        "predict_f = zarr.open('prediction.zarr')\n",
        "\n",
        "raw = test_f['raw/0'][:]\n",
        "labels = test_f['labels/0'][:]\n",
        "\n",
        "pred_affs = predict_f['pred_affs/0'][:]\n",
        "pred_lsds = predict_f['pred_lsds/0'][:]\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "            1,\n",
        "            4,\n",
        "            figsize=(20, 6),\n",
        "            sharex=True,\n",
        "            sharey=True,\n",
        "            squeeze=False)\n",
        "\n",
        "axes[0][0].imshow(np.squeeze(raw), cmap='gray')\n",
        "axes[0][1].imshow(create_lut(np.squeeze(labels)))\n",
        "axes[0][2].imshow(np.squeeze(pred_affs[0:1,:,:]), cmap='jet')\n",
        "axes[0][3].imshow(np.squeeze(pred_lsds[0:1,:,:]), cmap='jet')\n",
        "axes[0][3].imshow(np.squeeze(pred_lsds[1:2,:,:]), cmap='jet', alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "X6n9frU5W7rT"
      },
      "source": [
        "#@title watershed wrappers\n",
        "def watershed_from_boundary_distance(\n",
        "        boundary_distances,\n",
        "        boundary_mask,\n",
        "        return_seeds=False,\n",
        "        id_offset=0,\n",
        "        min_seed_distance=10):\n",
        "\n",
        "    max_filtered = maximum_filter(boundary_distances, min_seed_distance)\n",
        "    maxima = max_filtered==boundary_distances\n",
        "    seeds, n = label(maxima)\n",
        "\n",
        "    print(f\"Found {n} fragments\")\n",
        "\n",
        "    if n == 0:\n",
        "        return np.zeros(boundary_distances.shape, dtype=np.uint64), id_offset\n",
        "\n",
        "    seeds[seeds!=0] += id_offset\n",
        "\n",
        "    fragments = watershed(\n",
        "        boundary_distances.max() - boundary_distances,\n",
        "        seeds,\n",
        "        mask=boundary_mask)\n",
        "\n",
        "    ret = (fragments.astype(np.uint64), n + id_offset)\n",
        "    if return_seeds:\n",
        "        ret = ret + (seeds.astype(np.uint64),)\n",
        "\n",
        "    return ret\n",
        "\n",
        "def watershed_from_affinities(\n",
        "        affs,\n",
        "        max_affinity_value=1.0,\n",
        "        fragments_in_xy=True,\n",
        "        return_seeds=False,\n",
        "        min_seed_distance=10,\n",
        "        labels_mask=None):\n",
        "\n",
        "    mean_affs = 0.5*(affs[1] + affs[2])\n",
        "    depth = mean_affs.shape[0]\n",
        "\n",
        "    fragments = np.zeros(mean_affs.shape, dtype=np.uint64)\n",
        "    if return_seeds:\n",
        "        seeds = np.zeros(mean_affs.shape, dtype=np.uint64)\n",
        "\n",
        "    id_offset = 0\n",
        "\n",
        "    for z in range(depth):\n",
        "\n",
        "        boundary_mask = mean_affs[z]>0.5*max_affinity_value\n",
        "        boundary_distances = distance_transform_edt(boundary_mask)\n",
        "\n",
        "        if labels_mask is not None:\n",
        "\n",
        "            boundary_mask *= labels_mask.astype(bool)\n",
        "\n",
        "        ret = watershed_from_boundary_distance(\n",
        "            boundary_distances,\n",
        "            boundary_mask,\n",
        "            return_seeds=return_seeds,\n",
        "            id_offset=id_offset,\n",
        "            min_seed_distance=min_seed_distance)\n",
        "\n",
        "        fragments[z] = ret[0]\n",
        "        if return_seeds:\n",
        "            seeds[z] = ret[2]\n",
        "\n",
        "        id_offset = ret[1]\n",
        "\n",
        "    ret = (fragments, id_offset)\n",
        "    if return_seeds:\n",
        "        ret += (seeds,)\n",
        "\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0WnGY9MHXApc"
      },
      "source": [
        "#@title segmentation wrapper\n",
        "def get_segmentation(affinities, threshold, labels_mask=None):\n",
        "\n",
        "    fragments = watershed_from_affinities(\n",
        "            affinities,\n",
        "            labels_mask=labels_mask)[0]\n",
        "\n",
        "    thresholds = [threshold]\n",
        "\n",
        "    generator = waterz.agglomerate(\n",
        "        affs=affinities.astype(np.float32),\n",
        "        fragments=fragments,\n",
        "        thresholds=thresholds,\n",
        "    )\n",
        "\n",
        "    segmentation = next(generator)\n",
        "\n",
        "    return segmentation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pbplTCgXFfh"
      },
      "source": [
        "affs = daisy.open_ds(\"prediction.zarr\", \"pred_affs/0\")\n",
        "labels = daisy.open_ds(\"testing_data.zarr\", \"labels/0\")\n",
        "\n",
        "# affs shape: c, h, w (2, h, w)\n",
        "# labels shape: d, h, w (fake d (1))\n",
        "\n",
        "# get affs meta data\n",
        "roi = affs.roi\n",
        "voxel_size = affs.voxel_size\n",
        "offset = affs.roi.get_begin()\n",
        "\n",
        "# get labels back to 2d daisy array\n",
        "labels.materialize()\n",
        "\n",
        "labels_roi = daisy.Roi(\n",
        "        (labels.roi.get_begin()[0], labels.roi.get_begin()[1]),\n",
        "        (labels.roi.get_shape()[0], labels.roi.get_shape()[1]))\n",
        "\n",
        "labels = daisy.Array(\n",
        "        labels.data.reshape(\n",
        "            labels.shape[1],\n",
        "            labels.shape[2]),\n",
        "        labels_roi,\n",
        "        voxel_size)\n",
        "\n",
        "# intersect labels with affs roi (affs were predicted on smaller roi)\n",
        "labels = labels[roi]\n",
        "\n",
        "# convert to numpy arrays\n",
        "affs = affs.to_ndarray()\n",
        "labels = labels.to_ndarray()\n",
        "\n",
        "# watershed assumes 3d arrays, create fake channel dim\n",
        "affs = np.stack([\n",
        "    np.zeros_like(affs[0]),\n",
        "    affs[0],\n",
        "    affs[1]]\n",
        ")\n",
        "\n",
        "# affs shape: 3, h, w\n",
        "\n",
        "# waterz agglomerate requires 4d affs (c, d, h, w) - add fake z dim\n",
        "affs = np.expand_dims(affs, axis=1)\n",
        "\n",
        "#affs shape: 3, 1, h, w\n",
        "\n",
        "#just test a 0.5 threshold. higher thresholds will merge more, lower thresholds will split more\n",
        "threshold = 0.5\n",
        "\n",
        "segmentation = get_segmentation(affs, threshold, labels_mask=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0TBihU-YuBK"
      },
      "source": [
        "fig, axes = plt.subplots(\n",
        "            1,\n",
        "            5,\n",
        "            figsize=(20, 6),\n",
        "            sharex=True,\n",
        "            sharey=True,\n",
        "            squeeze=False)\n",
        "\n",
        "axes[0][0].imshow(np.squeeze(raw), cmap='gray')\n",
        "axes[0][1].imshow(create_lut(np.squeeze(labels)))\n",
        "axes[0][2].imshow(np.squeeze(pred_affs[0:1,:,:] + pred_affs[1:2,:,:]), cmap='jet')\n",
        "axes[0][3].imshow(np.squeeze(pred_lsds[0:1,:,:]), cmap='jet')\n",
        "axes[0][3].imshow(np.squeeze(pred_lsds[1:2,:,:]), cmap='jet', alpha=0.5)\n",
        "axes[0][4].imshow(create_lut(np.squeeze(segmentation)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}