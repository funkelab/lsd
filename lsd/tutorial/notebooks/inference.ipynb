{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxd06q3vZzlm"
      },
      "source": [
        "*  Before starting, click \"Runtime\" in the top panel, select \"Change runtime type\" and then choose \"GPU\"\n",
        "\n",
        "*  This tutorial follows the mtlsd tutorial, and is therefore condensed. Check out the mtlsd tutorial (**train_mtlsd.ipynb**) if there is any confusion throughout\n",
        "\n",
        "*  Try running each cell consecutively to see what is happening before changing things around\n",
        "\n",
        "*  Some cells are collapsed by default, these are generally utility functions or are expanded by defaullt in a previous tutorial. Double click to expand/collapse\n",
        "\n",
        "*  sometimes colab can be slow, if this happens you may need to restart the runtime. also, you generally can only run one session at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFzbgV1YZ2n0",
        "cellView": "form"
      },
      "source": [
        "#@title install packages + repos\n",
        "\n",
        "# packages\n",
        "!pip install matplotlib\n",
        "!pip install torch\n",
        "!pip install zarr\n",
        "\n",
        "# repos\n",
        "!pip install git+https://github.com/funkey/gunpowder.git\n",
        "!pip install git+https://github.com/funkelab/funlib.learn.torch.git\n",
        "!pip install git+https://github.com/funkey/waterz.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UzsrNvAZ5LF",
        "cellView": "form"
      },
      "source": [
        "#@title import packages\n",
        "\n",
        "import gunpowder as gp\n",
        "import h5py\n",
        "import io\n",
        "import logging\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import requests\n",
        "import torch\n",
        "import zarr\n",
        "\n",
        "from funlib.learn.torch.models import UNet, ConvPass\n",
        "from gunpowder.torch import Predict\n",
        "\n",
        "%matplotlib inline\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG7bYaaKgvGc"
      },
      "source": [
        "# so we can load a presaved checkpoint in a subsequent cell\n",
        "!git clone https://github.com/funkelab/lsd.git\n",
        "!cd lsd && git checkout tutorial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6exTbzR9Z_uV",
        "cellView": "form"
      },
      "source": [
        "#@title utility function to view labels\n",
        "\n",
        "# matplotlib uses a default shader\n",
        "# we need to recolor as unique objects\n",
        "\n",
        "def create_lut(labels):\n",
        "\n",
        "    max_label = np.max(labels)\n",
        "\n",
        "    lut = np.random.randint(\n",
        "            low=0,\n",
        "            high=255,\n",
        "            size=(int(max_label + 1), 3),\n",
        "            dtype=np.uint64)\n",
        "\n",
        "    lut = np.append(\n",
        "            lut,\n",
        "            np.zeros(\n",
        "                (int(max_label + 1), 1),\n",
        "                dtype=np.uint8) + 255,\n",
        "            axis=1)\n",
        "\n",
        "    lut[0] = 0\n",
        "    colored_labels = lut[labels]\n",
        "\n",
        "    return colored_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al-0cyZlaC57",
        "cellView": "form"
      },
      "source": [
        "#@title utility  function to download / save data as zarr\n",
        "def create_data(\n",
        "    url, \n",
        "    name, \n",
        "    offset, \n",
        "    resolution,\n",
        "    sections=None,\n",
        "    squeeze=True):\n",
        "\n",
        "  in_f = h5py.File(io.BytesIO(requests.get(url).content), 'r')\n",
        "\n",
        "  raw = in_f['volumes/raw']\n",
        "  labels = in_f['volumes/labels/neuron_ids']\n",
        "  \n",
        "  f = zarr.open(name, 'a')\n",
        "\n",
        "  if sections is None:\n",
        "    sections=range(raw.shape[0]-1)\n",
        "\n",
        "  for i, r in enumerate(sections):\n",
        "\n",
        "    print(f'Writing data for section {r}')\n",
        "\n",
        "    raw_slice = raw[r:r+1,:,:]\n",
        "    labels_slice = labels[r:r+1,:,:]\n",
        "\n",
        "    if squeeze:\n",
        "      raw_slice = np.squeeze(raw_slice)\n",
        "      labels_slice = np.squeeze(labels_slice)\n",
        "\n",
        "    f[f'raw/{i}'] = raw_slice\n",
        "    f[f'labels/{i}'] = labels_slice\n",
        "\n",
        "    f[f'raw/{i}'].attrs['offset'] = offset\n",
        "    f[f'raw/{i}'].attrs['resolution'] = resolution\n",
        "\n",
        "    f[f'labels/{i}'].attrs['offset'] = offset\n",
        "    f[f'labels/{i}'].attrs['resolution'] = resolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiLtfH8NaLdm"
      },
      "source": [
        "# fetch a random section\n",
        "\n",
        "create_data(\n",
        "    'https://cremi.org/static/data/sample_A_20160501.hdf',\n",
        "    'testing_data.zarr',\n",
        "    offset=[0,0],\n",
        "    resolution=[4,4],\n",
        "    sections=random.sample(range(0,124),1),\n",
        "    squeeze=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6efMx1KYq_A"
      },
      "source": [
        "voxel_size = gp.Coordinate((4, 4))\n",
        "\n",
        "input_shape = gp.Coordinate((164, 164))\n",
        "output_shape = gp.Coordinate((124, 124))\n",
        "\n",
        "input_size = input_shape * voxel_size\n",
        "output_size = output_shape * voxel_size\n",
        "\n",
        "# total roi of image to predict on\n",
        "total_roi = gp.Coordinate((1250,1250))*voxel_size\n",
        "\n",
        "num_fmaps=12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkxBq8BYchlf",
        "cellView": "form"
      },
      "source": [
        "#@title create mtlsd model\n",
        "\n",
        "class MtlsdModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.unet = UNet(\n",
        "            in_channels=1,\n",
        "            num_fmaps=num_fmaps,\n",
        "            fmap_inc_factor=5,\n",
        "            downsample_factors=[\n",
        "                [2, 2],\n",
        "                [2, 2]],\n",
        "            kernel_size_down=[\n",
        "                [[3, 3], [3, 3]],\n",
        "                [[3, 3], [3, 3]],\n",
        "                [[3, 3], [3, 3]]],\n",
        "            kernel_size_up=[\n",
        "                [[3, 3], [3, 3]],\n",
        "                [[3, 3], [3, 3]]])\n",
        "\n",
        "        self.lsd_head = ConvPass(num_fmaps, 6, [[1, 1]], activation='Sigmoid')\n",
        "        self.aff_head = ConvPass(num_fmaps, 2, [[1, 1]], activation='Sigmoid')\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        z = self.unet(input)\n",
        "        lsds = self.lsd_head(z)\n",
        "        affs = self.aff_head(z)\n",
        "\n",
        "        return lsds, affs\n",
        "\n",
        "class WeightedMSELoss(torch.nn.MSELoss):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(WeightedMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, lsds_prediction, lsds_target, lsds_weights, affs_prediction, affs_target, affs_weights,):\n",
        "\n",
        "        loss1 = super(WeightedMSELoss, self).forward(\n",
        "                lsds_prediction*lsds_weights,\n",
        "                lsds_target*lsds_weights)\n",
        "\n",
        "        loss2 = super(WeightedMSELoss, self).forward(\n",
        "            affs_prediction*affs_weights,\n",
        "            affs_target*affs_weights)\n",
        "        \n",
        "        return loss1 + loss2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3yhJQJRRj4X"
      },
      "source": [
        "def predict(\n",
        "    checkpoint,\n",
        "    raw_file,\n",
        "    raw_dataset,\n",
        "    out_file,\n",
        "    out_lsds,\n",
        "    out_affs):\n",
        "  \n",
        "  raw = gp.ArrayKey('RAW')\n",
        "  pred_lsds = gp.ArrayKey('PRED_LSDS')\n",
        "  pred_affs = gp.ArrayKey('PRED_AFFS')\n",
        "\n",
        "  if os.path.exists(out_file):\n",
        "    mode='r+'\n",
        "  else:\n",
        "    mode='w'\n",
        "\n",
        "  of = zarr.open(out_file, mode=mode)\n",
        "  rd = zarr.open(raw_file)[raw_dataset]\n",
        "\n",
        "  # create output datasets to write to\n",
        "  for ds in [out_lsds, out_affs]:\n",
        "      if ds not in of:\n",
        "          if 'lsd' in ds:\n",
        "              dims = 6\n",
        "          else:\n",
        "              dims = 2\n",
        "          out_ds = of.create_dataset(\n",
        "                  ds,\n",
        "                  shape= (dims,) + np.squeeze(rd).shape,\n",
        "                  dtype=np.float32)\n",
        "          out_ds.attrs['resolution'] = voxel_size\n",
        "\n",
        "  scan_request = gp.BatchRequest()\n",
        "\n",
        "  scan_request.add(raw, input_size)\n",
        "  scan_request.add(pred_lsds, output_size)\n",
        "  scan_request.add(pred_affs, output_size)\n",
        "\n",
        "  source = gp.ZarrSource(\n",
        "              raw_file,\n",
        "          {\n",
        "              raw: raw_dataset\n",
        "          },\n",
        "          {\n",
        "              raw: gp.ArraySpec(interpolatable=True)\n",
        "          })\n",
        "\n",
        "  model = MtlsdModel()\n",
        "\n",
        "  # set model to eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # add a predict node\n",
        "  predict = gp.torch.Predict(\n",
        "      model=model,\n",
        "      checkpoint=checkpoint,\n",
        "      inputs = {\n",
        "                'input': raw\n",
        "      },\n",
        "      outputs = {\n",
        "          0: pred_lsds,\n",
        "          1: pred_affs})\n",
        "  \n",
        "  # this will scan in chunks equal to the input/output sizes of the respective arrays\n",
        "  scan = gp.Scan(scan_request)\n",
        "\n",
        "  # write out data\n",
        "  write = gp.ZarrWrite(\n",
        "      dataset_names={\n",
        "          pred_lsds: out_lsds,\n",
        "          pred_affs: out_affs},\n",
        "      output_filename=out_file)\n",
        "  \n",
        "  pipeline = source\n",
        "  pipeline += gp.Normalize(raw)\n",
        "  pipeline += gp.Stack(1)\n",
        "  pipeline += predict\n",
        "  pipeline += scan\n",
        "  pipeline += gp.Squeeze([pred_lsds, pred_affs])\n",
        "  pipeline += write\n",
        "\n",
        "  predict_request = gp.BatchRequest()\n",
        "\n",
        "  # this lets us know to process the full image. we will scan over it until it is done\n",
        "  predict_request.add(raw, total_roi)\n",
        "  predict_request.add(pred_lsds, total_roi)\n",
        "  predict_request.add(pred_affs, total_roi)\n",
        "\n",
        "  with gp.build(pipeline):\n",
        "      pipeline.request_batch(predict_request)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYbHmh4dTL3V"
      },
      "source": [
        "checkpoint = 'lsd/lsd/tutorial/notebooks/model_checkpoint_50000' \n",
        "raw_file = 'testing_data.zarr'\n",
        "raw_dataset = 'raw/0'\n",
        "out_file = 'prediction.zarr'\n",
        "out_lsds = 'pred_lsds/0'\n",
        "out_affs = 'pred_affs/0'\n",
        "\n",
        "predict(\n",
        "    checkpoint,\n",
        "    raw_file,\n",
        "    raw_dataset,\n",
        "    out_file,\n",
        "    out_lsds,\n",
        "    out_affs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sqKklgfUx-c"
      },
      "source": [
        "test_f = zarr.open('testing_data.zarr')\n",
        "predict_f = zarr.open('prediction.zarr')\n",
        "\n",
        "raw = test_f['raw/0'][:]\n",
        "labels = test_f['labels/0'][:]\n",
        "\n",
        "pred_affs = predict_f['pred_affs/0'][:]\n",
        "pred_lsds = predict_f['pred_lsds/0'][:]\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "            1,\n",
        "            4,\n",
        "            figsize=(20, 6),\n",
        "            sharex=True,\n",
        "            sharey=True,\n",
        "            squeeze=False)\n",
        "\n",
        "# view predictions (for lsds we will just view the mean offset component)\n",
        "axes[0][0].imshow(np.squeeze(raw), cmap='gray')\n",
        "axes[0][1].imshow(create_lut(np.squeeze(labels)))\n",
        "axes[0][2].imshow(np.squeeze(pred_affs[0:1,:,:]), cmap='jet')\n",
        "axes[0][3].imshow(np.squeeze(pred_lsds[0:1,:,:]), cmap='jet')\n",
        "axes[0][3].imshow(np.squeeze(pred_lsds[1:2,:,:]), cmap='jet', alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srpG9JSYbL7v"
      },
      "source": [
        "*  see how to generate a segmentation in **segment.ipynb**"
      ]
    }
  ]
}